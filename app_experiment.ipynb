{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ab09bb3",
   "metadata": {},
   "source": [
    "All necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27c5c9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arnab\\.conda\\envs\\rag-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pymupdf\n",
    "import llama_cpp\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a79533",
   "metadata": {},
   "source": [
    "Path of pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2010b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"D:\\\\personalCode\\\\ragAgentFitness\\\\sample.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b36529c",
   "metadata": {},
   "source": [
    "PDF text reading function, text splitting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1753748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf_text(path):\n",
    "    doc=pymupdf.open(path)\n",
    "    full_text=\"\"\n",
    "    for page in doc:\n",
    "        full_text+=page.get_text()\n",
    "    return full_text\n",
    "\n",
    "def split_into_chunks(text, chunk_size=500, overlap=50):\n",
    "    chunks=[]\n",
    "    for i in range(0, len(text), chunk_size-overlap):\n",
    "        chunks.append(text[i:i+chunk_size])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96185681",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=read_pdf_text(path)\n",
    "chunks=split_into_chunks(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478da314",
   "metadata": {},
   "source": [
    "Embedding and storage of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf9803b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder=SentenceTransformer('all-MiniLM-L6-v2')\n",
    "vectors=embedder.encode(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f05b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension=vectors[0].shape[0]\n",
    "index=faiss.IndexFlatL2(dimension)\n",
    "index.reset()\n",
    "index.add(np.array(vectors))\n",
    "\n",
    "id_to_text={i: chunk for i, chunk in enumerate(chunks)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11de15ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_chunks(query, top_k=3):\n",
    "    query_vec=embedder.encode([query])\n",
    "    D, I=index.search(np.array(query_vec), top_k)\n",
    "    return [id_to_text[i] for i in I[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d05f5f",
   "metadata": {},
   "source": [
    "Model being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e509a7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 81 key-value pairs and 255 tensors from D:\\personalCode\\ragAgentFitness\\Dolphin3.0-Llama3.2-3B-Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Dolphin 3.0 Llama 3.2 3B\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Cognitivecomputations\n",
      "llama_model_loader: - kv   4:                           general.basename str              = dolphin-3.0-Llama-3.2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   6:                            general.license str              = llama3.2\n",
      "llama_model_loader: - kv   7:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   8:                  general.base_model.0.name str              = Llama 3.2 3B\n",
      "llama_model_loader: - kv   9:          general.base_model.0.organization str              = Meta Llama\n",
      "llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/meta-llama/Lla...\n",
      "llama_model_loader: - kv  11:                      general.dataset.count u32              = 13\n",
      "llama_model_loader: - kv  12:                     general.dataset.0.name str              = Opc Sft Stage1\n",
      "llama_model_loader: - kv  13:             general.dataset.0.organization str              = OpenCoder LLM\n",
      "llama_model_loader: - kv  14:                 general.dataset.0.repo_url str              = https://huggingface.co/OpenCoder-LLM/...\n",
      "llama_model_loader: - kv  15:                     general.dataset.1.name str              = Opc Sft Stage2\n",
      "llama_model_loader: - kv  16:             general.dataset.1.organization str              = OpenCoder LLM\n",
      "llama_model_loader: - kv  17:                 general.dataset.1.repo_url str              = https://huggingface.co/OpenCoder-LLM/...\n",
      "llama_model_loader: - kv  18:                     general.dataset.2.name str              = Orca Agentinstruct 1M v1\n",
      "llama_model_loader: - kv  19:                  general.dataset.2.version str              = v1\n",
      "llama_model_loader: - kv  20:             general.dataset.2.organization str              = Microsoft\n",
      "llama_model_loader: - kv  21:                 general.dataset.2.repo_url str              = https://huggingface.co/microsoft/orca...\n",
      "llama_model_loader: - kv  22:                     general.dataset.3.name str              = Orca Math Word Problems 200k\n",
      "llama_model_loader: - kv  23:             general.dataset.3.organization str              = Microsoft\n",
      "llama_model_loader: - kv  24:                 general.dataset.3.repo_url str              = https://huggingface.co/microsoft/orca...\n",
      "llama_model_loader: - kv  25:                     general.dataset.4.name str              = Hermes Function Calling v1\n",
      "llama_model_loader: - kv  26:                  general.dataset.4.version str              = v1\n",
      "llama_model_loader: - kv  27:             general.dataset.4.organization str              = NousResearch\n",
      "llama_model_loader: - kv  28:                 general.dataset.4.repo_url str              = https://huggingface.co/NousResearch/h...\n",
      "llama_model_loader: - kv  29:                     general.dataset.5.name str              = NuminaMath CoT\n",
      "llama_model_loader: - kv  30:             general.dataset.5.organization str              = AI MO\n",
      "llama_model_loader: - kv  31:                 general.dataset.5.repo_url str              = https://huggingface.co/AI-MO/NuminaMa...\n",
      "llama_model_loader: - kv  32:                     general.dataset.6.name str              = NuminaMath TIR\n",
      "llama_model_loader: - kv  33:             general.dataset.6.organization str              = AI MO\n",
      "llama_model_loader: - kv  34:                 general.dataset.6.repo_url str              = https://huggingface.co/AI-MO/NuminaMa...\n",
      "llama_model_loader: - kv  35:                     general.dataset.7.name str              = Tulu 3 Sft Mixture\n",
      "llama_model_loader: - kv  36:             general.dataset.7.organization str              = Allenai\n",
      "llama_model_loader: - kv  37:                 general.dataset.7.repo_url str              = https://huggingface.co/allenai/tulu-3...\n",
      "llama_model_loader: - kv  38:                     general.dataset.8.name str              = Dolphin Coder\n",
      "llama_model_loader: - kv  39:             general.dataset.8.organization str              = Cognitivecomputations\n",
      "llama_model_loader: - kv  40:                 general.dataset.8.repo_url str              = https://huggingface.co/cognitivecompu...\n",
      "llama_model_loader: - kv  41:                     general.dataset.9.name str              = Smoltalk\n",
      "llama_model_loader: - kv  42:             general.dataset.9.organization str              = HuggingFaceTB\n",
      "llama_model_loader: - kv  43:                 general.dataset.9.repo_url str              = https://huggingface.co/HuggingFaceTB/...\n",
      "llama_model_loader: - kv  44:                    general.dataset.10.name str              = Samantha Data\n",
      "llama_model_loader: - kv  45:            general.dataset.10.organization str              = Cognitivecomputations\n",
      "llama_model_loader: - kv  46:                general.dataset.10.repo_url str              = https://huggingface.co/cognitivecompu...\n",
      "llama_model_loader: - kv  47:                    general.dataset.11.name str              = CodeFeedback Filtered Instruction\n",
      "llama_model_loader: - kv  48:            general.dataset.11.organization str              = M A P\n",
      "llama_model_loader: - kv  49:                general.dataset.11.repo_url str              = https://huggingface.co/m-a-p/CodeFeed...\n",
      "llama_model_loader: - kv  50:                    general.dataset.12.name str              = Code Feedback\n",
      "llama_model_loader: - kv  51:            general.dataset.12.organization str              = M A P\n",
      "llama_model_loader: - kv  52:                general.dataset.12.repo_url str              = https://huggingface.co/m-a-p/Code-Fee...\n",
      "llama_model_loader: - kv  53:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv  54:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv  55:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  56:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  57:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  58:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  59:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  60:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  61:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  62:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  63:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  64:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  65:                           llama.vocab_size u32              = 128258\n",
      "llama_model_loader: - kv  66:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  67:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  68:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  69:                      tokenizer.ggml.tokens arr[str,128258]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  70:                  tokenizer.ggml.token_type arr[i32,128258]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  71:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  72:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  73:                tokenizer.ggml.eos_token_id u32              = 128256\n",
      "llama_model_loader: - kv  74:            tokenizer.ggml.padding_token_id u32              = 128001\n",
      "llama_model_loader: - kv  75:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - kv  76:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  77:                      quantize.imatrix.file str              = /models_out/Dolphin3.0-Llama3.2-3B-GG...\n",
      "llama_model_loader: - kv  78:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  79:             quantize.imatrix.entries_count i32              = 196\n",
      "llama_model_loader: - kv  80:              quantize.imatrix.chunks_count i32              = 125\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type q5_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q5_K - Medium\n",
      "print_info: file size   = 2.16 GiB (5.76 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "load: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "load: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "load: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "load: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "load: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "load: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "load: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "load: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "load: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "load: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "load: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "load: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "load: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "load: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "load: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "load: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "load: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "load: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "load: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "load: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "load: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "load: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "load: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "load: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "load: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "load: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "load: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "load: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "load: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "load: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "load: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "load: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "load: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "load: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "load: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "load: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "load: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "load: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "load: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "load: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "load: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "load: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "load: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "load: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "load: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "load: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "load: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "load: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "load: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "load: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "load: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "load: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "load: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "load: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "load: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "load: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "load: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "load: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "load: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "load: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "load: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "load: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "load: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "load: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "load: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "load: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "load: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "load: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "load: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "load: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "load: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "load: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "load: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "load: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "load: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "load: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "load: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "load: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "load: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "load: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "load: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "load: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "load: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "load: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "load: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "load: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "load: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "load: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "load: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "load: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "load: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "load: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "load: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "load: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "load: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "load: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "load: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "load: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "load: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "load: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "load: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "load: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "load: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "load: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "load: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "load: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "load: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "load: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "load: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "load: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "load: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "load: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "load: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "load: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "load: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "load: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "load: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "load: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "load: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "load: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "load: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "load: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "load: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "load: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "load: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "load: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "load: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "load: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "load: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "load: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "load: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "load: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "load: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "load: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "load: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "load: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "load: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "load: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "load: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "load: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "load: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "load: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "load: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "load: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "load: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "load: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "load: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "load: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "load: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "load: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "load: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "load: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "load: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "load: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "load: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "load: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "load: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "load: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "load: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "load: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "load: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "load: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "load: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "load: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "load: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "load: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "load: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "load: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "load: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "load: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "load: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "load: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "load: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "load: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "load: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "load: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "load: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "load: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "load: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "load: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "load: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "load: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "load: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "load: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "load: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "load: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "load: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "load: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "load: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "load: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "load: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "load: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "load: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "load: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "load: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "load: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "load: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "load: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "load: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "load: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "load: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "load: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "load: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "load: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "load: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "load: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "load: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "load: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "load: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "load: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "load: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "load: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "load: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "load: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "load: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "load: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "load: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "load: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "load: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "load: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "load: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "load: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "load: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "load: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "load: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "load: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "load: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "load: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "load: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "load: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "load: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "load: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "load: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "load: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "load: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "load: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "load: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "load: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "load: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "load: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "load: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "load: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "load: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "load: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "load: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "load: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "load: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "load: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "load: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "load: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "load: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "load: control token: 128257 '<|im_start|>' is not marked as EOG\n",
      "load: special tokens cache size = 258\n",
      "load: token to piece cache size = 0.7999 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 3072\n",
      "print_info: n_layer          = 28\n",
      "print_info: n_head           = 24\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 3\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 8192\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 3B\n",
      "print_info: model params     = 3.21 B\n",
      "print_info: general.name     = Dolphin 3.0 Llama 3.2 3B\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128258\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128256 '<|im_end|>'\n",
      "print_info: EOT token        = 128256 '<|im_end|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: PAD token        = 128001 '<|end_of_text|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: EOG token        = 128256 '<|im_end|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 282 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  2207.11 MiB\n",
      "........................................................................................\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 8192\n",
      "llama_init_from_model: n_ctx_per_seq = 8192\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 500000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =   896.00 MiB\n",
      "llama_init_from_model: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   424.01 MiB\n",
      "llama_init_from_model: graph nodes  = 902\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.dataset.10.repo_url': 'https://huggingface.co/cognitivecomputations/samantha-data', 'general.name': 'Dolphin 3.0 Llama 3.2 3B', 'general.dataset.9.repo_url': 'https://huggingface.co/HuggingFaceTB/smoltalk', 'general.dataset.8.name': 'Dolphin Coder', 'general.architecture': 'llama', 'general.type': 'model', 'llama.context_length': '131072', 'general.organization': 'Cognitivecomputations', 'general.basename': 'dolphin-3.0-Llama-3.2', 'general.dataset.0.repo_url': 'https://huggingface.co/OpenCoder-LLM/opc-sft-stage1', 'general.dataset.12.name': 'Code Feedback', 'general.size_label': '3B', 'general.license': 'llama3.2', 'general.dataset.9.organization': 'HuggingFaceTB', 'general.base_model.count': '1', 'general.base_model.0.name': 'Llama 3.2 3B', 'general.base_model.0.organization': 'Meta Llama', 'llama.attention.value_length': '128', 'general.base_model.0.repo_url': 'https://huggingface.co/meta-llama/Llama-3.2-3B', 'general.dataset.2.name': 'Orca Agentinstruct 1M v1', 'general.dataset.11.organization': 'M A P', 'general.dataset.count': '13', 'llama.attention.key_length': '128', 'general.dataset.0.name': 'Opc Sft Stage1', 'llama.rope.freq_base': '500000.000000', 'general.dataset.5.repo_url': 'https://huggingface.co/AI-MO/NuminaMath-CoT', 'general.dataset.4.name': 'Hermes Function Calling v1', 'general.dataset.0.organization': 'OpenCoder LLM', 'general.dataset.1.name': 'Opc Sft Stage2', 'general.dataset.1.organization': 'OpenCoder LLM', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.dataset.1.repo_url': 'https://huggingface.co/OpenCoder-LLM/opc-sft-stage2', 'general.dataset.2.version': 'v1', 'general.dataset.2.organization': 'Microsoft', 'general.dataset.12.organization': 'M A P', 'general.dataset.2.repo_url': 'https://huggingface.co/microsoft/orca-agentinstruct-1M-v1', 'general.dataset.3.name': 'Orca Math Word Problems 200k', 'general.dataset.3.organization': 'Microsoft', 'general.dataset.3.repo_url': 'https://huggingface.co/microsoft/orca-math-word-problems-200k', 'general.dataset.10.organization': 'Cognitivecomputations', 'general.dataset.4.version': 'v1', 'general.dataset.11.name': 'CodeFeedback Filtered Instruction', 'general.dataset.4.organization': 'NousResearch', 'general.dataset.5.name': 'NuminaMath CoT', 'general.dataset.4.repo_url': 'https://huggingface.co/NousResearch/hermes-function-calling-v1', 'general.dataset.5.organization': 'AI MO', 'general.dataset.6.name': 'NuminaMath TIR', 'general.dataset.6.organization': 'AI MO', 'general.dataset.6.repo_url': 'https://huggingface.co/AI-MO/NuminaMath-TIR', 'general.dataset.7.name': 'Tulu 3 Sft Mixture', 'llama.feed_forward_length': '8192', 'general.dataset.7.organization': 'Allenai', 'general.dataset.7.repo_url': 'https://huggingface.co/allenai/tulu-3-sft-mixture', 'general.dataset.8.organization': 'Cognitivecomputations', 'general.dataset.8.repo_url': 'https://huggingface.co/cognitivecomputations/dolphin-coder', 'general.dataset.9.name': 'Smoltalk', 'general.dataset.10.name': 'Samantha Data', 'llama.block_count': '28', 'general.dataset.11.repo_url': 'https://huggingface.co/m-a-p/CodeFeedback-Filtered-Instruction', 'general.file_type': '17', 'llama.attention.head_count_kv': '8', 'tokenizer.ggml.eos_token_id': '128256', 'general.dataset.12.repo_url': 'https://huggingface.co/m-a-p/Code-Feedback', 'llama.embedding_length': '3072', 'llama.attention.head_count': '24', 'llama.vocab_size': '128258', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'llama-bpe', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.padding_token_id': '128001', 'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'general.quantization_version': '2', 'quantize.imatrix.file': '/models_out/Dolphin3.0-Llama3.2-3B-GGUF/Dolphin3.0-Llama3.2-3B.imatrix', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'quantize.imatrix.entries_count': '196', 'quantize.imatrix.chunks_count': '125'}\n",
      "Available chat formats from metadata: chat_template.default\n"
     ]
    }
   ],
   "source": [
    "model_path_gguf=\"D:\\\\personalCode\\\\ragAgentFitness\\\\Dolphin3.0-Llama3.2-3B-Q5_K_M.gguf\"\n",
    "model=llama_cpp.Llama(model_path=model_path_gguf, chat_format=\"llama-2\", n_ctx=8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c891863e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192\n"
     ]
    }
   ],
   "source": [
    "print(model.context_params.n_ctx) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6de4bad",
   "metadata": {},
   "source": [
    "User input and search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9565e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens used by the context: 464\n"
     ]
    }
   ],
   "source": [
    "user_query=input(\"Please enter query: \")\n",
    "context=\"\\n\\n\".join(search_chunks(user_query))\n",
    "\n",
    "context_tokens = model.tokenize(context.encode(\"utf-8\"))\n",
    "context_token_count = len(context_tokens)\n",
    "print(f\"Total tokens used by the context: {context_token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d69f389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   14894.28 ms\n",
      "llama_perf_context_print: prompt eval time =   14893.70 ms /   560 tokens (   26.60 ms per token,    37.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =   43092.42 ms /   511 runs   (   84.33 ms per token,    11.86 tokens per second)\n",
      "llama_perf_context_print:       total time =   58957.66 ms /  1071 tokens\n"
     ]
    }
   ],
   "source": [
    "final_prompt=f\"\"\"<|im_start|>system\n",
    "You are a helpful assistant. If the answer is not present in the context, print \"Insufficient context\" and nothing else. Structure your response in markdown, using bullet points or headings if appropriate. Ensure that if there is no relevant information, you provide \"Insufficient context\" and nothing else at all. <|im_end|>\n",
    "<|im_start|>user\n",
    "Use the following context to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{user_query}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "temp=0.7\n",
    "max_tokens=512\n",
    "\n",
    "response=model.create_completion(\n",
    "    prompt=final_prompt,\n",
    "    temperature=temp,\n",
    "    max_tokens=max_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb3713d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"**How to do push ups?**\n",
      "\n",
      "* Begin in a standard push-up position, with your hands directly under your shoulders and your feet on the ground.\n",
      "* Straighten your arms and legs, and push your butt as far into the air as possible.\n",
      "* Keeping your arms straight and your back in line, swoop your upper body down in an arc, sticking your chest out.\n",
      "* Slowly return to the starting position, maintaining control throughout the movement.\n",
      "\n",
      "* For those who don't think they can ever do a single good push-up, I'll show you how to easily work into it:\n",
      "\t+ Start with your hands on the ground, but your knees raised, forming a plank position. Do as many of these as you can.\n",
      "\t+ Once you can do several in a row without resting, lower your knees to the ground, and do push-ups.\n",
      "\t+ The more you practice, the easier it will become.\n",
      "\n",
      "* **Posture and Variations:**\n",
      "\t+ Keep your core tight throughout the movement. Imagine you're pulling your belly button towards your tailbone.\n",
      "\t+ If you're not ready for the standard push-up, you can start working up to it by placing your hands on an elevated surface.\n",
      "\t+ The higher the surface, the easier it gets. This is a superior method of working up to a standard push-up than simply putting your knees on the ground and doing push-ups, because it will help you develop the important core strength needed.\n",
      "\t+ Once you can do several standard push-ups, you can move onto variations like handstand push-ups, one-arm push-ups, and even insane strength tests like the planche push-up.\n",
      "\n",
      "* **Variations:**\n",
      "\t+ For a more challenging push-up, try the planche push-up. This involves laying flat on your back, crossing your arms in front of your chest, and doing a push-up from this position.\n",
      "\t+ Another variation is the handstand push-up. This involves standing on your hands with your feet together, and lowering your body down to the ground.\n",
      "\n",
      "* **Workouts:**\n",
      "\t+ For a few workouts that focus squarely on your pecs, try the following:\n",
      "\t\t- Pec Push Workout:\n",
      "\t\t\t- 3 sets of 15-20 reps of push-ups.\n",
      "\t\t- 2 sets of 10-15 reps of dips.\n",
      "\t\t- 1 set of 10-15 reps of incline push-ups.\n",
      "\t\t- 1 set of 10-15 reps of plate\n"
     ]
    }
   ],
   "source": [
    "assistant_reply=response['choices'][0]['text']\n",
    "assistant_reply=assistant_reply.replace(\"[/INST]\", \"\")\n",
    "print(assistant_reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1831027f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 6 prefix-match hit, remaining 1083 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14894.28 ms\n",
      "llama_perf_context_print: prompt eval time =   29017.74 ms /  1083 tokens (   26.79 ms per token,    37.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =     738.54 ms /     7 runs   (  105.51 ms per token,     9.48 tokens per second)\n",
      "llama_perf_context_print:       total time =   29770.26 ms /  1090 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant reply is not hallucinated.\n"
     ]
    }
   ],
   "source": [
    "hallucination_prompt = f\"\"\"<|im_start|>system\n",
    "You are an expert fact-checking assistant. Your job is to verify whether the assistant's answer is fully supported by the given context. If parts of the answer are not present in the context, clearly identify them. Be strict and objective in your judgment.<|im_end|>\n",
    "<|im_start|>user\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "{assistant_reply}\n",
    "\n",
    "Task:\n",
    "Determine whether the answer is hallucinated. List any parts of the answer that are not supported by the context. If answer is determined to not be hallucinated respond with \"Assistant reply is not hallucinated.\"<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "hal_response = model.create_completion(\n",
    "    prompt=hallucination_prompt,\n",
    "    temperature=temp,\n",
    "    max_tokens=256\n",
    ")\n",
    "assistant_hallucination = hal_response['choices'][0]['text']\n",
    "print(assistant_hallucination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76adb32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant reply is not hallucinated.\n"
     ]
    }
   ],
   "source": [
    "print(assistant_hallucination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd517867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1f47878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_semantic_entropy(responses, distance_threshold=1.0):\n",
    "    embeddings=embedder.encode(responses)\n",
    "\n",
    "    clustering=AgglomerativeClustering(n_clusters=None, distance_threshold=distance_threshold, linkage=\"average\")\n",
    "    labels=clustering.fit_predict(embeddings)\n",
    "\n",
    "    counts=Counter(labels)\n",
    "    total=sum(counts.values())\n",
    "    probabilities=[count/total for count in counts.values()]\n",
    "    entropy=-sum(p*math.log2(p) for p in probabilities)\n",
    "\n",
    "    return entropy, labels    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "003b2fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =   37345.64 ms /     2 tokens (18672.82 ms per token,     0.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8787.68 ms /   107 runs   (   82.13 ms per token,    12.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    9039.19 ms /   109 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   12717.80 ms /   153 runs   (   83.12 ms per token,    12.03 tokens per second)\n",
      "llama_perf_context_print:       total time =   12957.05 ms /   154 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   10497.39 ms /   136 runs   (   77.19 ms per token,    12.96 tokens per second)\n",
      "llama_perf_context_print:       total time =   10688.50 ms /   137 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   19790.43 ms /   256 runs   (   77.31 ms per token,    12.94 tokens per second)\n",
      "llama_perf_context_print:       total time =   20213.46 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    7303.28 ms /   106 runs   (   68.90 ms per token,    14.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    7438.07 ms /   107 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 0 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   18256.13 ms /   256 runs   (   71.31 ms per token,    14.02 tokens per second)\n",
      "llama_perf_context_print:       total time =   18630.24 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17783.84 ms /   256 runs   (   69.47 ms per token,    14.40 tokens per second)\n",
      "llama_perf_context_print:       total time =   18159.11 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17581.53 ms /   256 runs   (   68.68 ms per token,    14.56 tokens per second)\n",
      "llama_perf_context_print:       total time =   17944.06 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17502.06 ms /   256 runs   (   68.37 ms per token,    14.63 tokens per second)\n",
      "llama_perf_context_print:       total time =   17868.79 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17539.53 ms /   256 runs   (   68.51 ms per token,    14.60 tokens per second)\n",
      "llama_perf_context_print:       total time =   17919.36 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 1 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17588.98 ms /   256 runs   (   68.71 ms per token,    14.55 tokens per second)\n",
      "llama_perf_context_print:       total time =   17987.54 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   18952.18 ms /   256 runs   (   74.03 ms per token,    13.51 tokens per second)\n",
      "llama_perf_context_print:       total time =   19346.50 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   15246.87 ms /   207 runs   (   73.66 ms per token,    13.58 tokens per second)\n",
      "llama_perf_context_print:       total time =   15549.00 ms /   208 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17812.51 ms /   239 runs   (   74.53 ms per token,    13.42 tokens per second)\n",
      "llama_perf_context_print:       total time =   18162.48 ms /   240 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17122.14 ms /   256 runs   (   66.88 ms per token,    14.95 tokens per second)\n",
      "llama_perf_context_print:       total time =   17480.30 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 2 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   15368.43 ms /   229 runs   (   67.11 ms per token,    14.90 tokens per second)\n",
      "llama_perf_context_print:       total time =   15687.75 ms /   230 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17304.91 ms /   256 runs   (   67.60 ms per token,    14.79 tokens per second)\n",
      "llama_perf_context_print:       total time =   17673.46 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   10001.02 ms /   150 runs   (   66.67 ms per token,    15.00 tokens per second)\n",
      "llama_perf_context_print:       total time =   10188.75 ms /   151 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17821.73 ms /   256 runs   (   69.62 ms per token,    14.36 tokens per second)\n",
      "llama_perf_context_print:       total time =   18192.07 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     561.96 ms /     7 runs   (   80.28 ms per token,    12.46 tokens per second)\n",
      "llama_perf_context_print:       total time =     570.25 ms /     8 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 3 Entropy:0.7219280948873623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17974.56 ms /   256 runs   (   70.21 ms per token,    14.24 tokens per second)\n",
      "llama_perf_context_print:       total time =   18344.06 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17733.71 ms /   256 runs   (   69.27 ms per token,    14.44 tokens per second)\n",
      "llama_perf_context_print:       total time =   18104.17 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   12278.88 ms /   180 runs   (   68.22 ms per token,    14.66 tokens per second)\n",
      "llama_perf_context_print:       total time =   12519.40 ms /   181 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17483.53 ms /   256 runs   (   68.30 ms per token,    14.64 tokens per second)\n",
      "llama_perf_context_print:       total time =   17847.97 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    5572.20 ms /    83 runs   (   67.13 ms per token,    14.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    5671.76 ms /    84 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 4 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   16432.41 ms /   239 runs   (   68.75 ms per token,    14.54 tokens per second)\n",
      "llama_perf_context_print:       total time =   16769.88 ms /   240 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17509.40 ms /   256 runs   (   68.40 ms per token,    14.62 tokens per second)\n",
      "llama_perf_context_print:       total time =   17868.61 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17431.70 ms /   256 runs   (   68.09 ms per token,    14.69 tokens per second)\n",
      "llama_perf_context_print:       total time =   17797.50 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   11655.12 ms /   173 runs   (   67.37 ms per token,    14.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   11876.37 ms /   174 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17655.72 ms /   256 runs   (   68.97 ms per token,    14.50 tokens per second)\n",
      "llama_perf_context_print:       total time =   18031.47 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 5 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17564.17 ms /   256 runs   (   68.61 ms per token,    14.58 tokens per second)\n",
      "llama_perf_context_print:       total time =   17936.09 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17553.13 ms /   256 runs   (   68.57 ms per token,    14.58 tokens per second)\n",
      "llama_perf_context_print:       total time =   17928.47 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   10220.63 ms /   151 runs   (   67.69 ms per token,    14.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   10419.14 ms /   152 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    5483.97 ms /    80 runs   (   68.55 ms per token,    14.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    5584.03 ms /    81 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    9519.00 ms /   138 runs   (   68.98 ms per token,    14.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    9693.73 ms /   139 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 6 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17659.09 ms /   256 runs   (   68.98 ms per token,    14.50 tokens per second)\n",
      "llama_perf_context_print:       total time =   18036.77 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17590.30 ms /   256 runs   (   68.71 ms per token,    14.55 tokens per second)\n",
      "llama_perf_context_print:       total time =   17951.69 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    4430.63 ms /    65 runs   (   68.16 ms per token,    14.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    4505.79 ms /    66 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17524.02 ms /   256 runs   (   68.45 ms per token,    14.61 tokens per second)\n",
      "llama_perf_context_print:       total time =   17890.40 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17304.65 ms /   256 runs   (   67.60 ms per token,    14.79 tokens per second)\n",
      "llama_perf_context_print:       total time =   17661.94 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 7 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   15157.72 ms /   223 runs   (   67.97 ms per token,    14.71 tokens per second)\n",
      "llama_perf_context_print:       total time =   15463.54 ms /   224 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   11963.14 ms /   174 runs   (   68.75 ms per token,    14.54 tokens per second)\n",
      "llama_perf_context_print:       total time =   12190.30 ms /   175 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17519.91 ms /   256 runs   (   68.44 ms per token,    14.61 tokens per second)\n",
      "llama_perf_context_print:       total time =   17885.89 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17621.98 ms /   256 runs   (   68.84 ms per token,    14.53 tokens per second)\n",
      "llama_perf_context_print:       total time =   17989.92 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17321.36 ms /   256 runs   (   67.66 ms per token,    14.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   17690.37 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 8 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17668.03 ms /   256 runs   (   69.02 ms per token,    14.49 tokens per second)\n",
      "llama_perf_context_print:       total time =   18032.19 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   12303.00 ms /   179 runs   (   68.73 ms per token,    14.55 tokens per second)\n",
      "llama_perf_context_print:       total time =   12543.75 ms /   180 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   10894.36 ms /   161 runs   (   67.67 ms per token,    14.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   11107.41 ms /   162 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   18186.24 ms /   256 runs   (   71.04 ms per token,    14.08 tokens per second)\n",
      "llama_perf_context_print:       total time =   18560.89 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17440.58 ms /   256 runs   (   68.13 ms per token,    14.68 tokens per second)\n",
      "llama_perf_context_print:       total time =   17810.97 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 9 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   11799.52 ms /   172 runs   (   68.60 ms per token,    14.58 tokens per second)\n",
      "llama_perf_context_print:       total time =   12021.55 ms /   173 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    8181.62 ms /   120 runs   (   68.18 ms per token,    14.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    8330.74 ms /   121 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17454.63 ms /   256 runs   (   68.18 ms per token,    14.67 tokens per second)\n",
      "llama_perf_context_print:       total time =   17818.92 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17528.66 ms /   256 runs   (   68.47 ms per token,    14.60 tokens per second)\n",
      "llama_perf_context_print:       total time =   17891.82 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    7670.32 ms /   114 runs   (   67.28 ms per token,    14.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    7807.83 ms /   115 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 10 Entropy:1.3709505944546687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17413.22 ms /   256 runs   (   68.02 ms per token,    14.70 tokens per second)\n",
      "llama_perf_context_print:       total time =   17779.21 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17524.81 ms /   256 runs   (   68.46 ms per token,    14.61 tokens per second)\n",
      "llama_perf_context_print:       total time =   17891.23 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17348.63 ms /   256 runs   (   67.77 ms per token,    14.76 tokens per second)\n",
      "llama_perf_context_print:       total time =   17704.22 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17385.02 ms /   256 runs   (   67.91 ms per token,    14.73 tokens per second)\n",
      "llama_perf_context_print:       total time =   17758.43 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17404.24 ms /   256 runs   (   67.99 ms per token,    14.71 tokens per second)\n",
      "llama_perf_context_print:       total time =   17778.62 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 11 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17578.67 ms /   256 runs   (   68.67 ms per token,    14.56 tokens per second)\n",
      "llama_perf_context_print:       total time =   17947.02 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17580.13 ms /   256 runs   (   68.67 ms per token,    14.56 tokens per second)\n",
      "llama_perf_context_print:       total time =   17940.84 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17417.31 ms /   256 runs   (   68.04 ms per token,    14.70 tokens per second)\n",
      "llama_perf_context_print:       total time =   17779.78 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17384.21 ms /   256 runs   (   67.91 ms per token,    14.73 tokens per second)\n",
      "llama_perf_context_print:       total time =   17754.22 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17415.24 ms /   256 runs   (   68.03 ms per token,    14.70 tokens per second)\n",
      "llama_perf_context_print:       total time =   17776.64 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 12 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17582.99 ms /   256 runs   (   68.68 ms per token,    14.56 tokens per second)\n",
      "llama_perf_context_print:       total time =   17943.51 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    6250.90 ms /    92 runs   (   67.94 ms per token,    14.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    6360.52 ms /    93 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17664.07 ms /   256 runs   (   69.00 ms per token,    14.49 tokens per second)\n",
      "llama_perf_context_print:       total time =   18035.73 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17561.57 ms /   256 runs   (   68.60 ms per token,    14.58 tokens per second)\n",
      "llama_perf_context_print:       total time =   17931.61 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17215.71 ms /   256 runs   (   67.25 ms per token,    14.87 tokens per second)\n",
      "llama_perf_context_print:       total time =   17571.64 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 13 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17427.20 ms /   256 runs   (   68.08 ms per token,    14.69 tokens per second)\n",
      "llama_perf_context_print:       total time =   17794.16 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17537.77 ms /   256 runs   (   68.51 ms per token,    14.60 tokens per second)\n",
      "llama_perf_context_print:       total time =   17902.00 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17329.79 ms /   256 runs   (   67.69 ms per token,    14.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   17693.74 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17300.29 ms /   256 runs   (   67.58 ms per token,    14.80 tokens per second)\n",
      "llama_perf_context_print:       total time =   17662.73 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    7920.34 ms /   118 runs   (   67.12 ms per token,    14.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    8063.66 ms /   119 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 14 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   10752.09 ms /   157 runs   (   68.48 ms per token,    14.60 tokens per second)\n",
      "llama_perf_context_print:       total time =   10951.34 ms /   158 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17583.14 ms /   256 runs   (   68.68 ms per token,    14.56 tokens per second)\n",
      "llama_perf_context_print:       total time =   17948.52 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17284.35 ms /   256 runs   (   67.52 ms per token,    14.81 tokens per second)\n",
      "llama_perf_context_print:       total time =   17646.06 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17616.56 ms /   256 runs   (   68.81 ms per token,    14.53 tokens per second)\n",
      "llama_perf_context_print:       total time =   17973.80 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   11033.91 ms /   161 runs   (   68.53 ms per token,    14.59 tokens per second)\n",
      "llama_perf_context_print:       total time =   11246.94 ms /   162 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 15 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17691.42 ms /   256 runs   (   69.11 ms per token,    14.47 tokens per second)\n",
      "llama_perf_context_print:       total time =   18066.57 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   14377.52 ms /   206 runs   (   69.79 ms per token,    14.33 tokens per second)\n",
      "llama_perf_context_print:       total time =   14664.74 ms /   207 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   14153.45 ms /   197 runs   (   71.84 ms per token,    13.92 tokens per second)\n",
      "llama_perf_context_print:       total time =   14430.38 ms /   198 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   16603.43 ms /   236 runs   (   70.35 ms per token,    14.21 tokens per second)\n",
      "llama_perf_context_print:       total time =   16966.83 ms /   237 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   18247.39 ms /   256 runs   (   71.28 ms per token,    14.03 tokens per second)\n",
      "llama_perf_context_print:       total time =   18635.12 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 16 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17499.92 ms /   256 runs   (   68.36 ms per token,    14.63 tokens per second)\n",
      "llama_perf_context_print:       total time =   17873.42 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   12234.55 ms /   181 runs   (   67.59 ms per token,    14.79 tokens per second)\n",
      "llama_perf_context_print:       total time =   12471.75 ms /   182 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     149.59 ms /     2 runs   (   74.80 ms per token,    13.37 tokens per second)\n",
      "llama_perf_context_print:       total time =     152.92 ms /     3 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17416.15 ms /   256 runs   (   68.03 ms per token,    14.70 tokens per second)\n",
      "llama_perf_context_print:       total time =   17783.22 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     486.03 ms /     7 runs   (   69.43 ms per token,    14.40 tokens per second)\n",
      "llama_perf_context_print:       total time =     495.45 ms /     8 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 17 Entropy:1.3709505944546687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   10068.52 ms /   149 runs   (   67.57 ms per token,    14.80 tokens per second)\n",
      "llama_perf_context_print:       total time =   10256.30 ms /   150 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17407.23 ms /   256 runs   (   68.00 ms per token,    14.71 tokens per second)\n",
      "llama_perf_context_print:       total time =   17770.63 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17403.40 ms /   256 runs   (   67.98 ms per token,    14.71 tokens per second)\n",
      "llama_perf_context_print:       total time =   17773.82 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17453.93 ms /   256 runs   (   68.18 ms per token,    14.67 tokens per second)\n",
      "llama_perf_context_print:       total time =   17825.65 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17393.15 ms /   256 runs   (   67.94 ms per token,    14.72 tokens per second)\n",
      "llama_perf_context_print:       total time =   17761.13 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 18 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17441.41 ms /   256 runs   (   68.13 ms per token,    14.68 tokens per second)\n",
      "llama_perf_context_print:       total time =   17807.15 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   15478.50 ms /   228 runs   (   67.89 ms per token,    14.73 tokens per second)\n",
      "llama_perf_context_print:       total time =   15801.59 ms /   229 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   16384.00 ms /   241 runs   (   67.98 ms per token,    14.71 tokens per second)\n",
      "llama_perf_context_print:       total time =   16725.63 ms /   242 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17473.93 ms /   256 runs   (   68.26 ms per token,    14.65 tokens per second)\n",
      "llama_perf_context_print:       total time =   17842.20 ms /   257 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14684.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17557.10 ms /   256 runs   (   68.58 ms per token,    14.58 tokens per second)\n",
      "llama_perf_context_print:       total time =   17924.44 ms /   257 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 19 Entropy:-0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    semantic_response=[model.create_completion(prompt=user_query, temperature=temp, max_tokens=256)['choices'][0]['text'] for _ in range(5)]\n",
    "    entropy, labels=compute_semantic_entropy(semantic_response, distance_threshold=1.0)\n",
    "    print(f\"At iteration {i} Entropy:{entropy}\")\n",
    "    if entropy>=1.5:\n",
    "        print(f\"iteration {i} detected possible hallucination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00d0b2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 6 prefix-match hit, remaining 554 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14894.28 ms\n",
      "llama_perf_context_print: prompt eval time =   14753.43 ms /   554 tokens (   26.63 ms per token,    37.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21498.60 ms /   255 runs   (   84.31 ms per token,    11.86 tokens per second)\n",
      "llama_perf_context_print:       total time =   36652.24 ms /   809 tokens\n",
      "Llama.generate: 559 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14894.28 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     379.05 ms /     4 runs   (   94.76 ms per token,    10.55 tokens per second)\n",
      "llama_perf_context_print:       total time =     385.26 ms /     5 tokens\n",
      "Llama.generate: 559 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14894.28 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   21344.26 ms /   256 runs   (   83.38 ms per token,    11.99 tokens per second)\n",
      "llama_perf_context_print:       total time =   21743.95 ms /   257 tokens\n",
      "Llama.generate: 559 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14894.28 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     338.12 ms /     4 runs   (   84.53 ms per token,    11.83 tokens per second)\n",
      "llama_perf_context_print:       total time =     343.62 ms /     5 tokens\n",
      "Llama.generate: 559 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14894.28 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   21725.08 ms /   256 runs   (   84.86 ms per token,    11.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   22133.36 ms /   257 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy:  0.971\n",
      "Response likely grounded\n"
     ]
    }
   ],
   "source": [
    "prompt=final_prompt\n",
    "semantic_response=[model.create_completion(prompt=prompt, temperature=temp, max_tokens=256)['choices'][0]['text'] for _ in range(5)]\n",
    "\n",
    "entropy, labels=compute_semantic_entropy(semantic_response)\n",
    "print(f\"Entropy: {entropy: .3f}\")\n",
    "\n",
    "if entropy>1.5:\n",
    "    print(\"Possible hallucination.\")\n",
    "\n",
    "else:\n",
    "    print(\"Response likely grounded\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
